import math
import numpy as np
from tqdm import tqdm
from collections import defaultdict
import pickle
import os
from sklearn.preprocessing import MinMaxScaler
from utils import get_item_user_time_dict
import collections
import faiss
import torch


# å®šä¹‰ç”¨æˆ·æ´»è·ƒåº¦æƒé‡
def get_user_activate_degree_dict(all_click_df):
    all_click_df_ = all_click_df.groupby('user_id')['click_article_id'].count().reset_index()

    # ç”¨æˆ·æ´»è·ƒåº¦å½’ä¸€åŒ–
    mm = MinMaxScaler()
    all_click_df_['click_article_id'] = mm.fit_transform(all_click_df_[['click_article_id']])
    user_activate_degree_dict = dict(zip(all_click_df_['user_id'], all_click_df_['click_article_id']))

    return user_activate_degree_dict

# UserCFç®—æ³•
def usercf_sim(all_click_df, user_activate_degree_dict, save_path, use_cache=True):
    """
    ç”¨æˆ·ç›¸ä¼¼æ€§çŸ©é˜µè®¡ç®— + ç¼“å­˜æœºåˆ¶ï¼ˆæ”¯æŒç‰ˆæœ¬è·¯å¾„ï¼‰

    :param all_click_df: ç”¨æˆ·ç‚¹å‡»æ—¥å¿—
    :param user_activate_degree_dict: ç”¨æˆ·æ´»è·ƒåº¦å­—å…¸
    :param save_path: ç¼“å­˜è·¯å¾„ï¼ˆå¯ä¸ºç›®å½•æˆ–å…·ä½“æ–‡ä»¶åï¼‰
    :param use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
    :return: ç”¨æˆ·ç›¸ä¼¼åº¦çŸ©é˜µ u2u_sim_
    """

    # === è·¯å¾„å¤„ç†ï¼šè‹¥ä¸ºç›®å½•ï¼Œæ‹¼æ¥é»˜è®¤æ–‡ä»¶å ===
    if os.path.splitext(save_path)[1] == '':
        save_path = os.path.join(save_path, 'usercf_u2u_sim.pkl')

    os.makedirs(os.path.dirname(save_path), exist_ok=True)

    # === ç¼“å­˜åŠ è½½é€»è¾‘ ===
    if use_cache and os.path.exists(save_path):
        print(f"[usercf_sim] âœ… Using cached file: {save_path}")  # ä½¿ç”¨ç¼“å­˜æ–‡ä»¶
        with open(save_path, 'rb') as f:
            return pickle.load(f)

    print("[usercf_sim] ğŸš§ Recomputing user similarity matrix...")  # æ­£åœ¨é‡æ–°è®¡ç®—ç”¨æˆ·ç›¸ä¼¼åº¦çŸ©é˜µ...

    # === æ­£å¼è®¡ç®— ===
    item_user_time_dict = get_item_user_time_dict(all_click_df)
    u2u_sim = {}
    user_cnt = defaultdict(int)

    for item, user_time_list in tqdm(item_user_time_dict.items()):
        for u, click_time in user_time_list:
            user_cnt[u] += 1
            u2u_sim.setdefault(u, {})
            for v, click_time in user_time_list:
                if u == v:
                    continue
                u2u_sim[u].setdefault(v, 0)
                # ç”¨æˆ·æ´»è·ƒåº¦åŠ æƒï¼ˆå¯è°ƒï¼‰
                activate_weight = 100 * 0.5 * (user_activate_degree_dict[u] + user_activate_degree_dict[v])
                u2u_sim[u][v] += activate_weight / math.log(len(user_time_list) + 1)

    # === å½’ä¸€åŒ– ===
    u2u_sim_ = u2u_sim.copy()
    for u, related_users in u2u_sim.items():
        for v, wij in related_users.items():
            u2u_sim_[u][v] = wij / math.sqrt(user_cnt[u] * user_cnt[v])

    # === ä¿å­˜ç›¸ä¼¼åº¦çŸ©é˜µ ===
    with open(save_path, 'wb') as f:
        pickle.dump(u2u_sim_, f)

    print(f"[usercf_sim] âœ… Similarity matrix saved to: {save_path}")  # ç›¸ä¼¼åº¦çŸ©é˜µå·²ä¿å­˜è‡³
    return u2u_sim_



# åŸºäºç”¨æˆ·çš„å¬å› u2u2i
def user_based_recommend(user_id, user_item_time_dict, u2u_sim, sim_user_topk, recall_item_num,
                         item_topk_click, item_created_time_dict, emb_i2i_sim):
    """åŸºäºç”¨æˆ·çš„å¬å›"""
    
    # ä¿®æ”¹è­¦å‘Šè¾“å‡ºæ–¹å¼
    if user_id not in u2u_sim:
        if user_id % 1000 == 0:  # æ¯1000ä¸ªç”¨æˆ·æ‰è¾“å‡ºä¸€æ¬¡
            print(f"âš ï¸ User ID {user_id} not in similarity matrix (only every 1000 shown)")  # ç”¨æˆ·ID ä¸åœ¨ç›¸ä¼¼åº¦çŸ©é˜µä¸­ (ä»…æ˜¾ç¤ºæ¯1000ä¸ª)
        # è¿”å›çƒ­é—¨ç‰©å“ä½œä¸ºåå¤‡æ–¹æ¡ˆ
        return [(item, -i-100) for i, item in enumerate(item_topk_click[:recall_item_num])]
    
    if user_id not in user_item_time_dict:
        print(f"âš ï¸ User {user_id} has no interaction history")  # ç”¨æˆ·æ²¡æœ‰å†å²äº¤äº’è®°å½•
        # è¿”å›çƒ­é—¨ç‰©å“ä½œä¸ºåå¤‡æ–¹æ¡ˆ
        return [(item, -i-100) for i, item in enumerate(item_topk_click[:recall_item_num])]
    
    # å†å²äº¤äº’
    user_item_time_list = user_item_time_dict[user_id]  # {item1: time1, item2: time2...}
    user_hist_items = set([i for i, t in user_item_time_list])  # å­˜åœ¨ä¸€ä¸ªç”¨æˆ·ä¸æŸç¯‡æ–‡ç« çš„å¤šæ¬¡äº¤äº’ï¼Œ è¿™é‡Œå¾—å»é‡

    items_rank = {}
    for sim_u, wuv in sorted(u2u_sim[user_id].items(), key=lambda x: x[1], reverse=True)[:sim_user_topk]:
        for i, click_time in user_item_time_dict[sim_u]:
            if i in user_hist_items:
                continue
            items_rank.setdefault(i, 0)

            loc_weight = 1.0
            content_weight = 1.0
            created_time_weight = 1.0

            # å½“å‰æ–‡ç« ä¸è¯¥ç”¨æˆ·çœ‹çš„å†å²æ–‡ç« è¿›è¡Œä¸€ä¸ªæƒé‡äº¤äº’
            for loc, (j, click_time) in enumerate(user_item_time_list):
                # ç‚¹å‡»æ—¶çš„ç›¸å¯¹ä½ç½®æƒé‡
                loc_weight += 0.9 ** (len(user_item_time_list) - loc)
                # å†…å®¹ç›¸ä¼¼æ€§æƒé‡
                if emb_i2i_sim.get(i, {}).get(j, None) is not None:
                    content_weight += emb_i2i_sim[i][j]
                if emb_i2i_sim.get(j, {}).get(i, None) is not None:
                    content_weight += emb_i2i_sim[j][i]

                # åˆ›å»ºæ—¶é—´å·®æƒé‡
                created_time_weight += np.exp(0.8 * np.abs(item_created_time_dict[i] - item_created_time_dict[j]))

            items_rank[i] += loc_weight * content_weight * created_time_weight * wuv

    # çƒ­åº¦è¡¥å…¨
    if len(items_rank) < recall_item_num:
        for i, item in enumerate(item_topk_click):
            if item in items_rank:  # ä¿®æ­£ï¼šæ£€æŸ¥æ˜¯å¦å·²åœ¨å­—å…¸ä¸­ï¼Œè€Œä¸æ˜¯æ£€æŸ¥æ˜¯å¦åœ¨å­—å…¸çš„items()ä¸­
                continue
            items_rank[item] = - i - 100  # éšä¾¿ç»™ä¸ªè´Ÿæ•°å°±è¡Œ
            if len(items_rank) == recall_item_num:
                break

    items_rank = sorted(items_rank.items(), key=lambda x: x[1], reverse=True)[:recall_item_num]

    return items_rank

# ============================
# âœ… UserCFç›¸ä¼¼åº¦è®¡ç®—ï¼ˆä¼ ç»Ÿç›¸ä¼¼ç”¨æˆ·ï¼‰
# ============================
def generate_usercf_recall_dict(click_df, user_item_time_dict, u2u_sim, sim_user_topk,
                              recall_item_num, item_topk_click, item_created_time_dict, 
                              emb_i2i_sim, save_path='./cache/', use_cache=True):
    """ç”ŸæˆåŸºäºç”¨æˆ·çš„å¬å›ç»“æœ"""
    
    # æ·»åŠ ç¼“å­˜è·¯å¾„
    cache_path = os.path.join(save_path, 'usercf_recall_dict.pkl')
    
    # æ£€æŸ¥ç¼“å­˜
    if use_cache and os.path.exists(cache_path):
        print(f"[generate_usercf_recall_dict] âœ… Using cache: {cache_path}")  # ä½¿ç”¨ç¼“å­˜
        with open(cache_path, 'rb') as f:
            return pickle.load(f)
    
    user_recall_items_dict = {}
    
    print("Generating user recall results...")  # ç”Ÿæˆç”¨æˆ·å¬å›ç»“æœ...
    total_users = len(click_df['user_id'].unique())
    missing_users = 0
    
    for user_id in tqdm(click_df['user_id'].unique()):
        user_recall_items_dict[user_id] = []
        
        # å¦‚æœç”¨æˆ·ä¸åœ¨ç›¸ä¼¼åº¦çŸ©é˜µä¸­ï¼Œè®°å½•å¹¶è·³è¿‡
        if user_id not in u2u_sim:
            missing_users += 1
            continue
            
        # è·å–ç›¸ä¼¼ç”¨æˆ·åŠå…¶ç›¸ä¼¼åº¦
        sim_users = u2u_sim[user_id]
        
        item_rank = {}
        for sim_user, sim_score in sim_users:
            # è·å–ç›¸ä¼¼ç”¨æˆ·çš„å†å²äº¤äº’
            if sim_user not in user_item_time_dict:
                continue
                
            sim_user_items = user_item_time_dict[sim_user]
            for item_id, _ in sim_user_items:
                if item_id in item_rank:
                    continue
                    
                item_rank[item_id] = sim_score
                
        # æŒ‰ç…§å¾—åˆ†æ’åºï¼Œå–å‰Nä¸ª
        item_rank_tuple = sorted(item_rank.items(), key=lambda x: x[1], reverse=True)[:recall_item_num]
        
        # å¦‚æœå¬å›æ•°é‡ä¸è¶³ï¼Œç”¨çƒ­é—¨ç‰©å“è¡¥å……
        if len(item_rank_tuple) < recall_item_num:
            for i, item in enumerate(item_topk_click):
                if item not in dict(item_rank_tuple):
                    item_rank_tuple.append((item, -i-100))
                if len(item_rank_tuple) >= recall_item_num:
                    break
                    
        user_recall_items_dict[user_id] = item_rank_tuple
    
    # æ‰“å°ç›¸ä¼¼åº¦çŸ©é˜µè¦†ç›–ç‡ç»Ÿè®¡
    coverage = (total_users - missing_users) / total_users
    print(f"\n[generate_usercf_recall_dict] User similarity coverage: {coverage:.4f}")  # ç”¨æˆ·ç›¸ä¼¼åº¦çŸ©é˜µè¦†ç›–ç‡
    print(f"Total users: {total_users}, missing users: {missing_users}")  # æ€»ç”¨æˆ·æ•° / ç¼ºå¤±ç”¨æˆ·æ•°
    
    # ä¿å­˜ç»“æœ
    os.makedirs(os.path.dirname(cache_path), exist_ok=True)
    with open(cache_path, 'wb') as f:
        pickle.dump(user_recall_items_dict, f)
    print(f"[generate_usercf_recall_dict] âœ… Recall results saved to: {cache_path}")  # å¬å›ç»“æœå·²ä¿å­˜è‡³
    
    return user_recall_items_dict


# ============================
# âœ… User Embedding ç›¸ä¼¼åº¦è®¡ç®—ï¼ˆYouTubeDNNç”¨æˆ·å‘é‡ï¼‰
# ============================
def generate_ucercf_embedding_recall_dict(click_df, user_emb_dict, save_path, topk):
    user_list = []
    user_emb_list = []
    for user_id, user_emb in user_emb_dict.items():
        user_list.append(user_id)
        user_emb_list.append(user_emb)

    user_index_2_rawid_dict = {k: v for k, v in zip(range(len(user_list)), user_list)}
    user_emb_np = np.array(user_emb_list, dtype=np.float32)

    index = faiss.IndexFlatIP(user_emb_np.shape[1])
    index.add(user_emb_np)
    sim, idx = index.search(user_emb_np, topk)

    user_sim_dict = collections.defaultdict(dict)
    for target_idx, sim_value_list, rele_idx_list in tqdm(zip(range(len(user_emb_np)), sim, idx)):
        target_raw_id = user_index_2_rawid_dict[target_idx]
        for rele_idx, sim_value in zip(rele_idx_list[1:], sim_value_list[1:]):
            rele_raw_id = user_index_2_rawid_dict[rele_idx]
            user_sim_dict[target_raw_id][rele_raw_id] = user_sim_dict.get(target_raw_id, {}).get(rele_raw_id, 0) + sim_value

    with open(os.path.join(save_path, 'youtube_u2u_sim.pkl'), 'wb') as f:
        pickle.dump(user_sim_dict, f)

    return user_sim_dict

# ä½¿ç”¨Embeddingçš„æ–¹å¼è·å–u2uçš„ç›¸ä¼¼æ€§çŸ©é˜µ
def u2u_embedding_sim(click_df, user_emb_dict, save_path='./cache/', topk=20, use_cache=True):
    """
    åŸºäºç”¨æˆ·åµŒå…¥è®¡ç®—ç”¨æˆ·ç›¸ä¼¼åº¦
    """
    cache_path = save_path if save_path.endswith('.pkl') else os.path.join(save_path, 'youtube_u2u_sim.pkl')
    
    if use_cache and os.path.exists(cache_path):
        print(f"[u2u_embedding_sim] âœ… Loaded user similarity cache: {cache_path}")  # åŠ è½½ç”¨æˆ·ç›¸ä¼¼åº¦ç¼“å­˜
        with open(cache_path, 'rb') as f:
            u2u_sim = pickle.load(f)
    else:
        print("[u2u_embedding_sim] Computing user similarity matrix...")  # è®¡ç®—ç”¨æˆ·ç›¸ä¼¼åº¦çŸ©é˜µ...
        
        # æ£€æŸ¥ç”¨æˆ·åµŒå…¥æ˜¯å¦ä¸ºç©º
        if not user_emb_dict:
            print("[u2u_embedding_sim] âš ï¸ User embedding dictionary is empty!")  # ç”¨æˆ·åµŒå…¥å­—å…¸ä¸ºç©ºï¼
            return {}
            
        # è·å–æ‰€æœ‰ç”¨æˆ·IDå’Œå¯¹åº”çš„åµŒå…¥
        print(f"[u2u_embedding_sim] User embedding count: {len(user_emb_dict)}")  # ç”¨æˆ·åµŒå…¥æ•°é‡
        
        # è½¬æ¢åµŒå…¥æ ¼å¼
        all_user_ids = []
        user_embeddings = []
        for user_id, emb in user_emb_dict.items():
            # ç¡®ä¿åµŒå…¥æ˜¯ä¸€ç»´æ•°ç»„
            if isinstance(emb, torch.Tensor):
                emb = emb.detach().cpu().numpy()
            if len(emb.shape) > 1:
                emb = emb.squeeze()
            
            all_user_ids.append(user_id)
            user_embeddings.append(emb)
        
        user_embeddings = np.array(user_embeddings, dtype=np.float32)
        print(f"[u2u_embedding_sim] Embedding matrix shape: {user_embeddings.shape}")  # åµŒå…¥çŸ©é˜µå½¢çŠ¶
        
        # å½’ä¸€åŒ–åµŒå…¥
        norms = np.linalg.norm(user_embeddings, axis=1, keepdims=True)
        user_embeddings = user_embeddings / norms
        
        # ä½¿ç”¨Faissè¿›è¡Œå¿«é€Ÿç›¸ä¼¼åº¦è®¡ç®—
        dim = user_embeddings.shape[1]
        index = faiss.IndexFlatIP(dim)
        index.add(user_embeddings)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        sim_scores, sim_idx = index.search(user_embeddings, topk + 1)
        
        # æ„å»ºç”¨æˆ·ç›¸ä¼¼åº¦å­—å…¸
        u2u_sim = {}
        for i, user_id in enumerate(all_user_ids):
            # è·³è¿‡ç¬¬ä¸€ä¸ªï¼ˆè‡ªå·±ï¼‰
            similar_users = [(all_user_ids[idx], float(score)) 
                           for idx, score in zip(sim_idx[i][1:], sim_scores[i][1:])]
            u2u_sim[user_id] = similar_users
        
        print(f"[u2u_embedding_sim] Computation done, user count: {len(u2u_sim)}")  # è®¡ç®—å®Œæˆï¼Œç”¨æˆ·æ•°
        
        # ä¿å­˜ç»“æœ
        os.makedirs(os.path.dirname(cache_path), exist_ok=True)
        with open(cache_path, 'wb') as f:
            pickle.dump(u2u_sim, f)
    
    # æ‰“å°ä¸€äº›ç»Ÿè®¡ä¿¡æ¯
    print(f"[u2u_embedding_sim] Users in similarity matrix: {len(u2u_sim)}")  # ç›¸ä¼¼åº¦çŸ©é˜µä¸­çš„ç”¨æˆ·æ•°
    if len(u2u_sim) > 0:
        sample_user = next(iter(u2u_sim))
        print(f"[u2u_embedding_sim] Sample - user {sample_user} similar user count: {len(u2u_sim[sample_user])}")  # æ ·ä¾‹ - ç”¨æˆ·...çš„ç›¸ä¼¼ç”¨æˆ·æ•°
        # æ‰“å°ä¸€äº›æ ·ä¾‹ç›¸ä¼¼åº¦
        print("\nSimilarity samples:")  # ç›¸ä¼¼åº¦æ ·ä¾‹
        for sim_user, sim_score in u2u_sim[sample_user][:3]:
            print(f"User {sample_user} -> User {sim_user}: {sim_score:.4f}")  # ç”¨æˆ·... -> ç”¨æˆ·...
    
    return u2u_sim
