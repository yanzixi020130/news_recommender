import os
import pickle
import pandas as pd
import numpy as np
from tqdm import tqdm
import faiss
import torch
from datetime import datetime

from utils import (
    get_user_item_time_dict,
    get_item_topk_click,
    get_item_info_dict
)
from data_processing import get_all_click_df, get_item_info_df, embdding_sim
from models.itemcf_recall import itemcf_sim, generate_itemcf_recall_dict, generate_itemcf_embedding_recall_dict
from models.usercf_recall import usercf_sim, generate_usercf_recall_dict, get_user_activate_degree_dict, u2u_embedding_sim
from models.YouTubeDNN_torch import YouTubeDNNModel, youtubednn_u2i_dict
from submission import submit

# ç›´æ¥ä½¿ç”¨ç›¸å¯¹è·¯å¾„
data_path = './data_raw/'
cache_dir = './cache/online'  # æ”¹ä¸ºç›¸å¯¹è·¯å¾„

# ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
os.makedirs(cache_dir, exist_ok=True)

print(f"Cache directory: {cache_dir}")  # ç¼“å­˜ç›®å½•

def get_youtube_recall(train_df, test_df, save_path, use_cache=False, epochs=10, batch_size=64, embedding_dim=64, recall_num=50):
    """
    ä½¿ç”¨PyTorchç‰ˆæœ¬çš„YouTubeDNNæ¨¡å‹ç”Ÿæˆç”¨æˆ·-ç‰©å“å¬å›è¡¨
    
    Args:
        train_df: è®­ç»ƒæ•°æ®
        test_df: æµ‹è¯•æ•°æ®
        save_path: ç»“æœä¿å­˜è·¯å¾„
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹å¤§å°
        embedding_dim: åµŒå…¥ç»´åº¦
        recall_num: å¬å›æ•°é‡
        
    Returns:
        ç”¨æˆ·-ç‰©å“å¬å›è¡¨ï¼Œæ ¼å¼ä¸º{ç”¨æˆ·ID: [(ç‰©å“ID, å¾—åˆ†), ...]}
    """
    # å®šä¹‰ç›¸å…³ç¼“å­˜è·¯å¾„
    cache_path = os.path.join(save_path, 'youtube_u2i_dict.pkl')
    model_path = os.path.join(save_path, 'youtube_dnn_model.pth')
    user_emb_path = os.path.join(save_path, 'user_youtube_emb.pkl') 
    item_emb_path = os.path.join(save_path, 'item_youtube_emb.pkl')
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(save_path, exist_ok=True)
    
    # æ£€æŸ¥å¬å›ç»“æœç¼“å­˜
    if use_cache and os.path.exists(cache_path):
        print(f"[get_youtube_recall] âœ… Using cache: {cache_path}")  # ä½¿ç”¨ç¼“å­˜
        with open(cache_path, 'rb') as f:
            return pickle.load(f)
    
    print("[get_youtube_recall] ğŸš€ Generating YouTubeDNN recall results...")  # ç”ŸæˆYouTubeDNNå¬å›ç»“æœ...
    
    # ä»…ä½¿ç”¨ç”¨æˆ·å’Œç‰©å“IDï¼Œç®€åŒ–å¤„ç†
    df = pd.concat([train_df, test_df], ignore_index=True)
    
    # é‡æ˜ å°„ç”¨æˆ·å’Œç‰©å“IDåˆ°è¿ç»­ç©ºé—´ï¼Œé¿å…ç´¢å¼•é—®é¢˜
    user_id_map = {uid: idx for idx, uid in enumerate(df['user_id'].unique())}
    item_id_map = {iid: idx for idx, iid in enumerate(df['click_article_id'].unique())}
    
    # è®°å½•é€†æ˜ å°„ï¼Œç”¨äºåç»­è¿˜åŸID
    user_id_reverse_map = {idx: uid for uid, idx in user_id_map.items()}
    item_id_reverse_map = {idx: iid for iid, idx in item_id_map.items()}
    
    # æ˜ å°„åçš„IDèŒƒå›´
    user_count = len(user_id_map)
    item_count = len(item_id_map)
    
    print(f"[get_youtube_recall] User count: {user_count}, item count: {item_count}")  # ç”¨æˆ·æ•°é‡ / ç‰©å“æ•°é‡
    
    # è·å–ç”¨æˆ·å†å²äº¤äº’ï¼Œä½¿ç”¨æ˜ å°„åçš„ID
    user_hist_dict = {}
    for user_id, group in df.groupby('user_id'):
        mapped_user_id = user_id_map[user_id]
        mapped_items = [item_id_map[item] for item in group.sort_values('click_timestamp')['click_article_id'].tolist()]
        user_hist_dict[mapped_user_id] = mapped_items
    
    # åˆ›å»ºæ¨¡å‹
    model = YouTubeDNNModel(user_count, item_count, embedding_dim=embedding_dim)
    
    # æ£€æŸ¥æ¨¡å‹ç¼“å­˜
    if use_cache and os.path.exists(model_path):
        print(f"[get_youtube_recall] âœ… Loaded pretrained model: {model_path}")  # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        model.load_state_dict(torch.load(model_path))
    
    # æ£€æŸ¥åµŒå…¥ç¼“å­˜
    if use_cache and os.path.exists(user_emb_path) and os.path.exists(item_emb_path):
        print(f"[get_youtube_recall] âœ… Loaded user and item embeddings")  # åŠ è½½ç”¨æˆ·å’Œç‰©å“åµŒå…¥
        with open(user_emb_path, 'rb') as f:
            user_embeddings = pickle.load(f)
        with open(item_emb_path, 'rb') as f:
            item_embeddings = pickle.load(f)
    else:
        # ç”Ÿæˆç”¨æˆ·å’Œç‰©å“çš„åµŒå…¥
        print("[get_youtube_recall] Computing user and item embeddings...")  # è®¡ç®—ç”¨æˆ·å’Œç‰©å“åµŒå…¥...
        model.eval()
        
        # ä¸ºæ‰€æœ‰ç‰©å“ç”ŸæˆåµŒå…¥ï¼ˆä½¿ç”¨æ˜ å°„åçš„IDï¼‰
        with torch.no_grad():
            all_item_ids = torch.LongTensor(list(range(item_count)))
            all_item_embs = model.get_item_embedding(all_item_ids).detach().numpy()
            normalized_item_embs = all_item_embs / np.linalg.norm(all_item_embs, axis=1, keepdims=True)
            
            # ä¿å­˜ç‰©å“åµŒå…¥å­—å…¸ï¼Œä½¿ç”¨åŸå§‹ID
            item_embeddings = {item_id_reverse_map[idx]: emb for idx, emb in enumerate(normalized_item_embs)}
            with open(item_emb_path, 'wb') as f:
                pickle.dump(item_embeddings, f)
        
        # è®¡ç®—ç”¨æˆ·åµŒå…¥
        user_embeddings = {}
        max_seq_len = 30
        
        with torch.no_grad():
            for mapped_user_id, hist_items in tqdm(user_hist_dict.items(), desc="Computing user embeddings"):
                if not hist_items:
                    continue
                    
                # æœ€å¤šä½¿ç”¨æœ€è¿‘30ä¸ªäº¤äº’
                hist_items = hist_items[-max_seq_len:] if len(hist_items) > max_seq_len else hist_items
                hist_len = len(hist_items)
                
                # å°†å†å²äº¤äº’è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥
                hist_tensor = torch.LongTensor(hist_items + [0] * (max_seq_len - hist_len))
                hist_tensor = hist_tensor.unsqueeze(0)  # å¢åŠ æ‰¹æ¬¡ç»´åº¦
                user_tensor = torch.LongTensor([mapped_user_id])
                seq_len = torch.LongTensor([hist_len])
                
                # è·å–ç”¨æˆ·åµŒå…¥
                try:
                    user_emb = model.get_user_embedding(user_tensor, hist_tensor, seq_len).numpy()
                    # ä¿å­˜æ—¶ä½¿ç”¨åŸå§‹ID
                    original_user_id = user_id_reverse_map[mapped_user_id]
                    user_embeddings[original_user_id] = user_emb.squeeze() / np.linalg.norm(user_emb)
                except Exception as e:
                    print(f"[get_youtube_recall] âš ï¸ Error processing embedding for user {mapped_user_id}: {str(e)}")  # å¤„ç†ç”¨æˆ·åµŒå…¥æ—¶å‡ºé”™
                    continue
        
        # ä¿å­˜ç”¨æˆ·åµŒå…¥
        with open(user_emb_path, 'wb') as f:
            pickle.dump(user_embeddings, f)
    
    # å‡†å¤‡å‘é‡æ£€ç´¢
    print("[get_youtube_recall] Using Faiss for vector retrieval...")  # ä½¿ç”¨Faissè¿›è¡Œå‘é‡æ£€ç´¢...
    user_ids = list(user_embeddings.keys())
    user_embs = np.array([user_embeddings[user_id] for user_id in user_ids], dtype=np.float32)
    
    item_ids = list(item_embeddings.keys())
    item_embs = np.array([item_embeddings[item_id] for item_id in item_ids], dtype=np.float32)
    
    # æ„å»ºç´¢å¼•
    index = faiss.IndexFlatIP(embedding_dim)
    index.add(np.ascontiguousarray(item_embs))
    
    # ä¸ºæµ‹è¯•é›†ä¸­çš„ç”¨æˆ·ç”Ÿæˆå¬å›ç»“æœ
    user_recall_items_dict = {}
    topk = recall_num  # æ¯ä¸ªç”¨æˆ·å¬å›æŒ‡å®šæ•°é‡çš„æ–‡ç« 
    
    # æ‰§è¡Œå‘é‡æ£€ç´¢
    sim, idx = index.search(np.ascontiguousarray(user_embs), topk)
    
    # æ„å»ºç”¨æˆ·å¬å›ç»“æœ
    for i, user_id in enumerate(user_ids):
        item_list = []
        for j, item_idx in enumerate(idx[i]):
            if item_idx < len(item_ids):
                item_id = item_ids[item_idx]
                score = float(sim[i][j])
                item_list.append((item_id, score))
        user_recall_items_dict[user_id] = item_list
    
    # ä¿å­˜å¬å›ç»“æœ
    with open(cache_path, 'wb') as f:
        pickle.dump(user_recall_items_dict, f)
    
    print(f"[get_youtube_recall] âœ… Recall results saved to: {cache_path}")  # å¬å›ç»“æœå·²ä¿å­˜è‡³
    return user_recall_items_dict

def combine_recall_results(user_multi_recall_dict, weight_dict=None, topk=25, save_path='cache/'):
    """
    åˆå¹¶å¤šè·¯å¬å›ç»“æœ
    
    Args:
        user_multi_recall_dict: å¤šè·¯å¬å›ç»“æœå­—å…¸ï¼Œæ ¼å¼ä¸º {å¬å›æ–¹æ³•: {ç”¨æˆ·ID: [(ç‰©å“ID, å¾—åˆ†), ...]}}
        weight_dict: å„è·¯å¬å›çš„æƒé‡å­—å…¸ï¼Œæ ¼å¼ä¸º {å¬å›æ–¹æ³•: æƒé‡å€¼}
        topk: æœ€ç»ˆè¿”å›çš„æ¨èç‰©å“æ•°é‡
        save_path: ç»“æœä¿å­˜è·¯å¾„
        
    Returns:
        åˆå¹¶åçš„å¬å›ç»“æœå­—å…¸ï¼Œæ ¼å¼ä¸º {ç”¨æˆ·ID: [(ç‰©å“ID, å¾—åˆ†), ...]}
    """
    final_recall_items_dict = {}
    
    # å¯¹æ¯ä¸€ç§å¬å›ç»“æœæŒ‰ç…§ç”¨æˆ·è¿›è¡Œå½’ä¸€åŒ–ï¼Œæ–¹ä¾¿åé¢å¤šç§å¬å›ç»“æœï¼Œç›¸åŒç”¨æˆ·çš„ç‰©å“ä¹‹é—´æƒé‡ç›¸åŠ 
    def norm_user_recall_items_sim(sorted_item_list):
        # å¦‚æœå†·å¯åŠ¨ä¸­æ²¡æœ‰æ–‡ç« æˆ–è€…åªæœ‰ä¸€ç¯‡æ–‡ç« ï¼Œç›´æ¥è¿”å›ï¼Œå‡ºç°è¿™ç§æƒ…å†µçš„åŸå› å¯èƒ½æ˜¯å†·å¯åŠ¨å¬å›çš„æ–‡ç« æ•°é‡å¤ªå°‘äº†ï¼Œ
        # åŸºäºè§„åˆ™ç­›é€‰ä¹‹åå°±æ²¡æœ‰æ–‡ç« äº†, è¿™é‡Œè¿˜å¯ä»¥åšä¸€äº›å…¶ä»–çš„ç­–ç•¥æ€§çš„ç­›é€‰
        if len(sorted_item_list) < 2:
            return sorted_item_list
        
        min_sim = sorted_item_list[-1][1]
        max_sim = sorted_item_list[0][1]
        
        norm_sorted_item_list = []
        for item, score in sorted_item_list:
            if max_sim > 0:
                norm_score = 1.0 * (score - min_sim) / (max_sim - min_sim) if max_sim > min_sim else 1.0
            else:
                norm_score = 0.0
            norm_sorted_item_list.append((item, norm_score))
            
        return norm_sorted_item_list
    
    print('Combining multiple recall results...')  # å¤šè·¯å¬å›åˆå¹¶...
    for method, user_recall_items in tqdm(user_multi_recall_dict.items()):
        print(method + '...')  # å¬å›æ–¹æ³•åç§°
        # åœ¨è®¡ç®—æœ€ç»ˆå¬å›ç»“æœçš„æ—¶å€™ï¼Œä¹Ÿå¯ä»¥ä¸ºæ¯ä¸€ç§å¬å›ç»“æœè®¾ç½®ä¸€ä¸ªæƒé‡
        if weight_dict is None:
            recall_method_weight = 1
        else:
            recall_method_weight = weight_dict.get(method, 1)
        
        for user_id, sorted_item_list in user_recall_items.items(): # è¿›è¡Œå½’ä¸€åŒ–
            user_recall_items[user_id] = norm_user_recall_items_sim(sorted_item_list)
        
        for user_id, sorted_item_list in user_recall_items.items():
            final_recall_items_dict.setdefault(user_id, {})
            for item, score in sorted_item_list:
                final_recall_items_dict[user_id].setdefault(item, 0)
                final_recall_items_dict[user_id][item] += recall_method_weight * score  
    
    final_recall_items_dict_rank = {}
    # å¤šè·¯å¬å›æ—¶ä¹Ÿå¯ä»¥æ§åˆ¶æœ€ç»ˆçš„å¬å›æ•°é‡
    for user, recall_item_dict in final_recall_items_dict.items():
        final_recall_items_dict_rank[user] = sorted(recall_item_dict.items(), key=lambda x: x[1], reverse=True)[:topk]

    # ç¡®ä¿ä¿å­˜è·¯å¾„å­˜åœ¨
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    
    # å°†å¤šè·¯å¬å›åçš„æœ€ç»ˆç»“æœå­—å…¸ä¿å­˜åˆ°æœ¬åœ°
    with open(save_path, 'wb') as f:
        pickle.dump(final_recall_items_dict_rank, f)

    return final_recall_items_dict_rank

def online_predict(use_cache=True, recall_num=50, epochs=10, batch_size=32, embedding_dim=32):
    print(f"\nğŸ“‚ Current cache directory: {cache_dir}")  # å½“å‰ä½¿ç”¨çš„ç¼“å­˜ç›®å½•
    
    # æ£€æŸ¥ç°æœ‰ç¼“å­˜æ–‡ä»¶
    if os.path.exists(cache_dir):
        cache_files = os.listdir(cache_dir)
        print("\nExisting cache files:")  # ç°æœ‰ç¼“å­˜æ–‡ä»¶
        for file in cache_files:
            file_path = os.path.join(cache_dir, file)
            file_time = datetime.fromtimestamp(os.path.getmtime(file_path))
            print(f"  - {file} (updated: {file_time})")  # æ›´æ–°æ—¶é—´
    
    # ç»Ÿä¸€ä½¿ç”¨ä¸€ä¸ªæ¨¡å‹æ–‡ä»¶å
    model_cache = os.path.join(cache_dir, 'youtube_model.pth')  # ä½¿ç”¨å®é™…ä¿å­˜çš„æ–‡ä»¶å
    
    # å¦‚æœæ¨¡å‹ç¼“å­˜å­˜åœ¨ï¼Œç›´æ¥åŠ è½½
    if use_cache and os.path.exists(model_cache):
        print(f"\nâœ… Found model cache: {model_cache}")  # å‘ç°æ¨¡å‹ç¼“å­˜
        print(f"   Cache time: {datetime.fromtimestamp(os.path.getmtime(model_cache))}")  # ç¼“å­˜æ—¶é—´
        print("âœ… Model cache found")  # å·²æˆåŠŸæ‰¾åˆ°æ¨¡å‹ç¼“å­˜
    
    os.makedirs(cache_dir, exist_ok=True)
    
    # Step 1: åŠ è½½æ•°æ®
    print("ğŸ“Œ Loading training and test data...")  # åŠ è½½è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®...
    train_df = get_all_click_df(data_path=data_path, offline=True)  # è®­ç»ƒæ•°æ®
    test_df = pd.read_csv(data_path + 'testA_click_log.csv')  # æµ‹è¯•æ•°æ®
    print(f"âœ… Train records: {len(train_df)}, test records: {len(test_df)}")  # è®­ç»ƒé›†...æµ‹è¯•é›†...
    
    # Step 2: å‡†å¤‡ç›¸å…³æ•°æ®
    item_info_df = get_item_info_df(data_path)
    item_type_dict, item_words_dict, item_created_time_dict = get_item_info_dict(item_info_df)
    user_item_time_dict = get_user_item_time_dict(train_df)
    item_topk_click = get_item_topk_click(train_df, k=50)
    
    # Step 3: åŠ è½½å¹¶æŠ½æ · embeddingï¼Œæ„å»º embedding ç›¸ä¼¼åº¦
    print("ğŸš€ Step 3: Loading article embeddings and sampling for similarity")  # åŠ è½½æ–‡ç«  embedding å¹¶æŠ½æ ·æ„å»ºç›¸ä¼¼åº¦
    emb_sample_n = 1000
    item_emb_df = pd.read_csv(data_path + '/articles_emb.csv').sample(n=emb_sample_n, random_state=42)
    emb_item_ids = set(item_emb_df['article_id'])
    click_df_for_emb = train_df[train_df['click_article_id'].isin(emb_item_ids)]
    
    # embedding_sim åªç”¨ click_df_for_embï¼Œè€Œä¸è¦æ±¡æŸ“ä¸»æµç¨‹çš„ train_df
    emb_i2i_sim = embdding_sim(click_df_for_emb, item_emb_df, save_path=cache_dir, topk=10)
    print(f"âœ… Step 3: Embedding similarity computation completed")  # å®Œæˆ embedding ç›¸ä¼¼åº¦è®¡ç®—
    
    # Step 4: é¦–å…ˆç”ŸæˆYouTubeDNNå¬å›å¹¶æå–ç”¨æˆ·åµŒå…¥
    print("ğŸ”„ Step 4.1: Generate YouTubeDNN recall and extract user embeddings...")  # ç”ŸæˆYouTubeDNNå¬å›å¹¶æå–ç”¨æˆ·åµŒå…¥...
    
    # åˆå¹¶æ•°æ®å¹¶å¢åŠ æ—¶é—´æˆ³æ’åº
    all_df = pd.concat([train_df, test_df], ignore_index=True)
    all_df = all_df.sort_values('click_timestamp')
    
    # å…ˆç”ŸæˆYouTubeDNNçš„å¬å›ç»“æœå’ŒåµŒå…¥
    youtube_recall_dict = youtubednn_u2i_dict(
        data=all_df,
        save_path=cache_dir,
        topk=recall_num,
        epochs=epochs,
        batch_size=batch_size,
        embedding_dim=embedding_dim
    )
    
    # ç„¶ååŠ è½½ç”Ÿæˆçš„ç”¨æˆ·åµŒå…¥
    user_emb_path = os.path.join(cache_dir, 'youtube_embeddings.pkl')  # æ³¨æ„è¿™é‡Œæ”¹ç”¨æ­£ç¡®çš„æ–‡ä»¶å
    if os.path.exists(user_emb_path):
        print(f"[online_predict] âœ… Loaded user embeddings: {user_emb_path}")  # åŠ è½½ç”¨æˆ·åµŒå…¥
        with open(user_emb_path, 'rb') as f:
            cache_data = pickle.load(f)
            user_embeddings = cache_data['user_embeddings']
    else:
        print("[online_predict] âš ï¸ User embedding file not found")  # æ— æ³•æ‰¾åˆ°ç”¨æˆ·åµŒå…¥æ–‡ä»¶
        user_embeddings = {}
    
    # Step 5: ç”Ÿæˆä¼ ç»ŸItemCFå¬å›
    print("ğŸ”„ Step 4.2: Generating traditional ItemCF recall...")  # ç”Ÿæˆä¼ ç»ŸItemCFå¬å›...
    
    # åˆå¹¶è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ¥è·å–å®Œæ•´çš„ç”¨æˆ·-ç‰©å“äº¤äº’å­—å…¸
    all_df = pd.concat([train_df, test_df], ignore_index=True)
    all_user_item_time_dict = get_user_item_time_dict(all_df)
    
    # ä½¿ç”¨åˆå¹¶åçš„æ•°æ®è®¡ç®—ç‰©å“ç›¸ä¼¼åº¦
    i2i_sim = itemcf_sim(
        all_df,  # ä½¿ç”¨åˆå¹¶åçš„æ•°æ®
        item_created_time_dict,
        save_path=os.path.join(cache_dir, 'itemcf_i2i_sim.pkl'),
        use_cache=use_cache
    )
    
    itemcf_recall_dict = generate_itemcf_recall_dict(
        val_df=test_df,
        user_item_time_dict=all_user_item_time_dict,
        i2i_sim=i2i_sim,
        sim_item_topk=10,
        recall_item_num=recall_num,
        item_topk_click=item_topk_click,
        item_created_time_dict=item_created_time_dict,
        emb_i2i_sim=emb_i2i_sim,
        save_path=os.path.join(cache_dir, 'itemcf_recall_dict.pkl'),
        use_cache=use_cache
    )
    
    # Step 6: ç”ŸæˆåŸºäºEmbeddingçš„ItemCFå¬å›
    print("ğŸ”„ Step 4.3: Generating embedding-based ItemCF recall...")  # ç”ŸæˆåŸºäºEmbeddingçš„ItemCFå¬å›...
    itemcf_emb_recall_dict = generate_itemcf_embedding_recall_dict(
        val_df=test_df,
        emb_i2i_sim=emb_i2i_sim,  # ç›´æ¥ä½¿ç”¨embeddingç›¸ä¼¼åº¦
        user_item_time_dict=all_user_item_time_dict,  # ä½¿ç”¨å®Œæ•´çš„ç”¨æˆ·-ç‰©å“äº¤äº’å­—å…¸
        sim_item_topk=10,
        recall_item_num=recall_num,
        item_topk_click=item_topk_click,
        item_created_time_dict=item_created_time_dict,
        save_path=os.path.join(cache_dir, 'itemcf_emb_recall_dict.pkl'),
        use_cache=use_cache
    )
    
    # Step 7: ç”ŸæˆåŸºäºEmbeddingçš„UserCFå¬å›
    print("\nğŸ”„ Step 4.4: Generating embedding-based UserCF recall...")  # ç”ŸæˆåŸºäºEmbeddingçš„UserCFå¬å›...
    
    # æ£€æŸ¥ç”¨æˆ·åµŒå…¥çš„å¯ç”¨æ€§
    if not user_embeddings:
        print("âš ï¸ Warning: user embeddings are empty")  # è­¦å‘Šï¼šç”¨æˆ·åµŒå…¥ä¸ºç©º
        print(f"User embedding count: {len(user_embeddings)}")  # ç”¨æˆ·åµŒå…¥æ•°é‡
    
    u2u_emb_sim = u2u_embedding_sim(
        click_df=test_df,  # æ³¨æ„è¿™é‡Œä½¿ç”¨test_dfè€Œä¸æ˜¯val_df
        user_emb_dict=user_embeddings,
        save_path=os.path.join(cache_dir, 'youtube_u2u_sim.pkl'),
        topk=20,
        use_cache=use_cache
    )
    
    # æ‰“å°ç”¨æˆ·ç›¸ä¼¼åº¦çŸ©é˜µçš„ç»Ÿè®¡ä¿¡æ¯
    print(f"\nUser similarity matrix stats:")  # ç”¨æˆ·ç›¸ä¼¼åº¦çŸ©é˜µç»Ÿè®¡
    print(f"Total users: {len(test_df['user_id'].unique())}")  # æ€»ç”¨æˆ·æ•°
    print(f"Users in similarity matrix: {len(u2u_emb_sim)}")  # ç›¸ä¼¼åº¦çŸ©é˜µä¸­çš„ç”¨æˆ·æ•°
    
    print("\nğŸ”„ Step 4.5: Generating embedding-based UserCF recall results...")  # ç”ŸæˆåŸºäºEmbeddingçš„UserCFçš„å¬å›ç»“æœ...
    usercf_emb_recall_dict = generate_usercf_recall_dict(
        click_df=test_df,  # æ³¨æ„è¿™é‡Œä½¿ç”¨test_dfè€Œä¸æ˜¯val_df
        user_item_time_dict=user_item_time_dict,
        u2u_sim=u2u_emb_sim,
        sim_user_topk=10,
        recall_item_num=recall_num,
        item_topk_click=item_topk_click,
        item_created_time_dict=item_created_time_dict,
        emb_i2i_sim=i2i_sim,
        save_path=cache_dir,  # æ·»åŠ ç¼“å­˜è·¯å¾„
        use_cache=use_cache   # ä½¿ç”¨ç¼“å­˜å‚æ•°
    )
    
    # Step 8: åˆå¹¶å¬å›ç»“æœ
    print("\nğŸ”„ Combining multiple recall results...")  # åˆå¹¶å¤šè·¯å¬å›ç»“æœ...
    # åˆ›å»ºå¤šè·¯å¬å›å­—å…¸
    user_multi_recall_dict = {
        'itemcf': itemcf_recall_dict,
        'itemcf_emb': itemcf_emb_recall_dict,
        'usercf_emb': usercf_emb_recall_dict,
        'youtube': youtube_recall_dict
    }
    
    # è®¾ç½®å„è·¯å¬å›çš„æƒé‡
    weight_dict = {
        'itemcf': 1.0,
        'itemcf_emb': 0.8,
        'usercf_emb': 0.9,
        'youtube': 1.2
    }
    
    # åˆå¹¶å¬å›ç»“æœ
    final_recall_dict = combine_recall_results(
        user_multi_recall_dict=user_multi_recall_dict,
        weight_dict=weight_dict,
        topk=recall_num,
        save_path=os.path.join(cache_dir, 'final_recall_dict.pkl')
    )
    
    # Step 9: ç”Ÿæˆæäº¤æ–‡ä»¶
    print("\nğŸ“ Generating submission file...")  # ç”Ÿæˆæäº¤æ–‡ä»¶...
    # å°†å¬å›ç»“æœè½¬æ¢ä¸ºDataFrameæ ¼å¼
    recall_list = []
    for user_id, items in final_recall_dict.items():
        # åªä¿ç•™testAä¸­çš„ç”¨æˆ·ï¼ˆ200000~249999ï¼‰
        if 200000 <= user_id <= 249999:
            for item_id, score in items:
                recall_list.append({
                    'user_id': user_id,
                    'click_article_id': item_id,
                    'pred_score': score
                })
    
    recall_df = pd.DataFrame(recall_list)
    
    # ç¡®ä¿resultsç›®å½•å­˜åœ¨
    os.makedirs('./results', exist_ok=True)
    
    # ä½¿ç”¨submission.pyä¸­çš„submitå‡½æ•°ç”Ÿæˆæäº¤æ–‡ä»¶
    submit(
        recall_df=recall_df,
        save_path='./results/',  # ä¿®æ”¹ä¿å­˜è·¯å¾„ä¸ºresultsæ–‡ä»¶å¤¹
        topk=5,
        model_name='multi_recall'
    )
    
    print("âœ… Submission file generated!")  # æäº¤æ–‡ä»¶ç”Ÿæˆå®Œæˆï¼

if __name__ == '__main__':
    # é…ç½®å‚æ•°
    use_cache = False  # å¼ºåˆ¶é‡æ–°è®­ç»ƒ
    recall_num = 50   # å¬å›æ•°é‡
    epochs = 20        # å¢åŠ è®­ç»ƒè½®æ•°
    batch_size = 128   # å¢åŠ æ‰¹æ¬¡å¤§å°
    embedding_dim = 64  # å¢åŠ åµŒå…¥ç»´åº¦
    
    # æ£€æŸ¥è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")  # ä½¿ç”¨è®¾å¤‡
    
    # è¿è¡Œåœ¨çº¿é¢„æµ‹
    online_predict(
        use_cache=use_cache,  # å¼ºåˆ¶é‡æ–°è®­ç»ƒ
        recall_num=recall_num,
        epochs=epochs,        # å¢åŠ è®­ç»ƒè½®æ•°
        batch_size=batch_size,   # å¢åŠ æ‰¹æ¬¡å¤§å°
        embedding_dim=embedding_dim  # å¢åŠ åµŒå…¥ç»´åº¦
    ) 