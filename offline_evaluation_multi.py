import os
import pickle
import pandas as pd
import numpy as np
from tqdm import tqdm
import faiss
import torch
from datetime import datetime

from utils import (
    get_user_item_time_dict,
    get_item_topk_click,
    get_item_info_dict
)
from data_processing import get_all_click_df, get_item_info_df, embdding_sim
from models.itemcf_recall import itemcf_sim, generate_itemcf_recall_dict, generate_itemcf_embedding_recall_dict
from models.usercf_recall import usercf_sim, generate_usercf_recall_dict, get_user_activate_degree_dict, u2u_embedding_sim
from models.YouTubeDNN_torch import YouTubeDNNModel, youtubednn_u2i_dict
from recall_evaluation import metrics_recall


data_path = './data_raw/'
cache_dir = './cache/offline_multi' 

# ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
os.makedirs(cache_dir, exist_ok=True)

def split_train_val(all_click_df):
    """
    å°†æ•°æ®é›†æŒ‰ç”¨æˆ·åˆ†å‰²ä¸ºè®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼Œæ¯ä¸ªç”¨æˆ·çš„æœ€åä¸€æ¬¡ç‚¹å‡»ä½œä¸ºéªŒè¯é›†
    
    Args:
        all_click_df: æ‰€æœ‰çš„ç‚¹å‡»æ•°æ®
        
    Returns:
        è®­ç»ƒé›†å’ŒéªŒè¯é›†
    """
    all_click_df = all_click_df.sort_values(by=['user_id', 'click_timestamp'])
    val_clicks = all_click_df.groupby('user_id').tail(1)
    train_clicks = all_click_df.drop(val_clicks.index)
    return train_clicks, val_clicks

def metrics_mrr(user_recall_items_dict, val_df, topk=5):
    """
    è®¡ç®—æ‰€æœ‰ç”¨æˆ·çš„ MRR@topkï¼ˆMean Reciprocal Rankï¼‰

    Args:
        user_recall_items_dict: {user_id: [(item_id, score), ...]}
        val_df: pd.DataFrameï¼ŒéªŒè¯é›†ä¸­æ¯ä¸ªç”¨æˆ·çš„æœ€åä¸€æ¬¡ç‚¹å‡»
        topk: intï¼Œåªè¯„ä¼°å‰ topk ä¸ªæ¨è
    """
    last_click_item_dict = dict(zip(val_df['user_id'], val_df['click_article_id']))
    total_score = 0
    user_cnt = 0

    for user, rec_list in user_recall_items_dict.items():
        if user not in last_click_item_dict:
            continue
        true_item = last_click_item_dict[user]
        mrr_score = 0

        for rank, (item, _) in enumerate(rec_list[:topk], start=1):
            if item == true_item:
                mrr_score += 1 / rank  # æ»¡è¶³ s(user,k)=1 æ—¶ï¼ŒåŠ ä¸Š 1/k
        total_score += mrr_score
        user_cnt += 1

    avg_mrr = round(total_score / user_cnt, 5) if user_cnt else 0.0
    print(f"ğŸ“Š Final offline MRR@{topk}: {avg_mrr}")  # æœ€ç»ˆç¦»çº¿ MRR
    return avg_mrr

def combine_recall_results(user_multi_recall_dict, weight_dict=None, topk=25, save_path='cache/'):
    """
    åˆå¹¶å¤šè·¯å¬å›ç»“æœ
    
    Args:
        user_multi_recall_dict: å¤šè·¯å¬å›ç»“æœå­—å…¸ï¼Œæ ¼å¼ä¸º {å¬å›æ–¹æ³•: {ç”¨æˆ·ID: [(ç‰©å“ID, å¾—åˆ†), ...]}}
        weight_dict: å„è·¯å¬å›çš„æƒé‡å­—å…¸ï¼Œæ ¼å¼ä¸º {å¬å›æ–¹æ³•: æƒé‡å€¼}
        topk: æœ€ç»ˆè¿”å›çš„æ¨èç‰©å“æ•°é‡
        save_path: ç»“æœä¿å­˜è·¯å¾„
        
    Returns:
        åˆå¹¶åçš„å¬å›ç»“æœå­—å…¸ï¼Œæ ¼å¼ä¸º {ç”¨æˆ·ID: [(ç‰©å“ID, å¾—åˆ†), ...]}
    """
    final_recall_items_dict = {}
    
    # å¯¹æ¯ä¸€ç§å¬å›ç»“æœæŒ‰ç…§ç”¨æˆ·è¿›è¡Œå½’ä¸€åŒ–ï¼Œæ–¹ä¾¿åé¢å¤šç§å¬å›ç»“æœï¼Œç›¸åŒç”¨æˆ·çš„ç‰©å“ä¹‹é—´æƒé‡ç›¸åŠ 
    def norm_user_recall_items_sim(sorted_item_list):
        # å¦‚æœå†·å¯åŠ¨ä¸­æ²¡æœ‰æ–‡ç« æˆ–è€…åªæœ‰ä¸€ç¯‡æ–‡ç« ï¼Œç›´æ¥è¿”å›ï¼Œå‡ºç°è¿™ç§æƒ…å†µçš„åŸå› å¯èƒ½æ˜¯å†·å¯åŠ¨å¬å›çš„æ–‡ç« æ•°é‡å¤ªå°‘äº†ï¼Œ
        # åŸºäºè§„åˆ™ç­›é€‰ä¹‹åå°±æ²¡æœ‰æ–‡ç« äº†, è¿™é‡Œè¿˜å¯ä»¥åšä¸€äº›å…¶ä»–çš„ç­–ç•¥æ€§çš„ç­›é€‰
        if len(sorted_item_list) < 2:
            return sorted_item_list
        
        min_sim = sorted_item_list[-1][1]
        max_sim = sorted_item_list[0][1]
        
        norm_sorted_item_list = []
        for item, score in sorted_item_list:
            if max_sim > 0:
                norm_score = 1.0 * (score - min_sim) / (max_sim - min_sim) if max_sim > min_sim else 1.0
            else:
                norm_score = 0.0
            norm_sorted_item_list.append((item, norm_score))
            
        return norm_sorted_item_list
    
    print('Combining multiple recall results...')  # å¤šè·¯å¬å›åˆå¹¶...
    for method, user_recall_items in tqdm(user_multi_recall_dict.items()):
        print(method + '...')  # å¬å›æ–¹æ³•åç§°
        # åœ¨è®¡ç®—æœ€ç»ˆå¬å›ç»“æœçš„æ—¶å€™ï¼Œä¹Ÿå¯ä»¥ä¸ºæ¯ä¸€ç§å¬å›ç»“æœè®¾ç½®ä¸€ä¸ªæƒé‡
        if weight_dict is None:
            recall_method_weight = 1
        else:
            recall_method_weight = weight_dict.get(method, 1)
        
        for user_id, sorted_item_list in user_recall_items.items(): # è¿›è¡Œå½’ä¸€åŒ–
            user_recall_items[user_id] = norm_user_recall_items_sim(sorted_item_list)
        
        for user_id, sorted_item_list in user_recall_items.items():
            final_recall_items_dict.setdefault(user_id, {})
            for item, score in sorted_item_list:
                final_recall_items_dict[user_id].setdefault(item, 0)
                final_recall_items_dict[user_id][item] += recall_method_weight * score  
    
    final_recall_items_dict_rank = {}
    # å¤šè·¯å¬å›æ—¶ä¹Ÿå¯ä»¥æ§åˆ¶æœ€ç»ˆçš„å¬å›æ•°é‡
    for user, recall_item_dict in final_recall_items_dict.items():
        final_recall_items_dict_rank[user] = sorted(recall_item_dict.items(), key=lambda x: x[1], reverse=True)[:topk]

    # ç¡®ä¿ä¿å­˜è·¯å¾„å­˜åœ¨
    os.makedirs(os.path.dirname(save_path), exist_ok=True)
    
    # å°†å¤šè·¯å¬å›åçš„æœ€ç»ˆç»“æœå­—å…¸ä¿å­˜åˆ°æœ¬åœ°
    with open(save_path, 'wb') as f:
        pickle.dump(final_recall_items_dict_rank, f)

    return final_recall_items_dict_rank

def get_youtube_recall(train_df, val_df, save_path, use_cache=False, epochs=10, batch_size=64, embedding_dim=64, recall_num=50):
    """
    ä½¿ç”¨PyTorchç‰ˆæœ¬çš„YouTubeDNNæ¨¡å‹ç”Ÿæˆç”¨æˆ·-ç‰©å“å¬å›è¡¨
    
    Args:
        train_df: è®­ç»ƒæ•°æ®
        val_df: éªŒè¯æ•°æ®
        save_path: ç»“æœä¿å­˜è·¯å¾„
        use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹å¤§å°
        embedding_dim: åµŒå…¥ç»´åº¦
        recall_num: å¬å›æ•°é‡
        
    Returns:
        ç”¨æˆ·-ç‰©å“å¬å›è¡¨ï¼Œæ ¼å¼ä¸º{ç”¨æˆ·ID: [(ç‰©å“ID, å¾—åˆ†), ...]}
    """
    # å®šä¹‰ç›¸å…³ç¼“å­˜è·¯å¾„
    cache_path = os.path.join(save_path, 'youtube_u2i_dict.pkl')
    model_path = os.path.join(save_path, 'youtube_dnn_model.pth')
    user_emb_path = os.path.join(save_path, 'user_youtube_emb.pkl') 
    item_emb_path = os.path.join(save_path, 'item_youtube_emb.pkl')
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(save_path, exist_ok=True)
    
    # æ£€æŸ¥å¬å›ç»“æœç¼“å­˜
    if use_cache and os.path.exists(cache_path):
        print(f"[get_youtube_recall] âœ… Using cache: {cache_path}")  # ä½¿ç”¨ç¼“å­˜
        with open(cache_path, 'rb') as f:
            return pickle.load(f)
    
    print("[get_youtube_recall] ğŸš€ Generating YouTubeDNN recall results...")  # ç”ŸæˆYouTubeDNNå¬å›ç»“æœ...
    
    # ä»…ä½¿ç”¨ç”¨æˆ·å’Œç‰©å“IDï¼Œç®€åŒ–å¤„ç†
    df = pd.concat([train_df, val_df], ignore_index=True)
    
    # é‡æ˜ å°„ç”¨æˆ·å’Œç‰©å“IDåˆ°è¿ç»­ç©ºé—´ï¼Œé¿å…ç´¢å¼•é—®é¢˜
    user_id_map = {uid: idx for idx, uid in enumerate(df['user_id'].unique())}
    item_id_map = {iid: idx for idx, iid in enumerate(df['click_article_id'].unique())}
    
    # è®°å½•é€†æ˜ å°„ï¼Œç”¨äºåç»­è¿˜åŸID
    user_id_reverse_map = {idx: uid for uid, idx in user_id_map.items()}
    item_id_reverse_map = {idx: iid for iid, idx in item_id_map.items()}
    
    # æ˜ å°„åçš„IDèŒƒå›´
    user_count = len(user_id_map)
    item_count = len(item_id_map)
    
    print(f"[get_youtube_recall] User count: {user_count}, item count: {item_count}")  # ç”¨æˆ·æ•°é‡ / ç‰©å“æ•°é‡
    
    # è·å–ç”¨æˆ·å†å²äº¤äº’ï¼Œä½¿ç”¨æ˜ å°„åçš„ID
    user_hist_dict = {}
    for user_id, group in df.groupby('user_id'):
        mapped_user_id = user_id_map[user_id]
        mapped_items = [item_id_map[item] for item in group.sort_values('click_timestamp')['click_article_id'].tolist()]
        user_hist_dict[mapped_user_id] = mapped_items
    
    # åˆ›å»ºæ¨¡å‹
    model = YouTubeDNNModel(user_count, item_count, embedding_dim=embedding_dim)
    
    # æ£€æŸ¥æ¨¡å‹ç¼“å­˜
    if use_cache and os.path.exists(model_path):
        print(f"[get_youtube_recall] âœ… Loaded pretrained model: {model_path}")  # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
        model.load_state_dict(torch.load(model_path))
    
    # æ£€æŸ¥åµŒå…¥ç¼“å­˜
    if use_cache and os.path.exists(user_emb_path) and os.path.exists(item_emb_path):
        print(f"[get_youtube_recall] âœ… Loaded user and item embeddings")  # åŠ è½½ç”¨æˆ·å’Œç‰©å“åµŒå…¥
        with open(user_emb_path, 'rb') as f:
            user_embeddings = pickle.load(f)
        with open(item_emb_path, 'rb') as f:
            item_embeddings = pickle.load(f)
    else:
        # ç”Ÿæˆç”¨æˆ·å’Œç‰©å“çš„åµŒå…¥
        print("[get_youtube_recall] Computing user and item embeddings...")  # è®¡ç®—ç”¨æˆ·å’Œç‰©å“åµŒå…¥...
        model.eval()
        
        # ä¸ºæ‰€æœ‰ç‰©å“ç”ŸæˆåµŒå…¥ï¼ˆä½¿ç”¨æ˜ å°„åçš„IDï¼‰
        with torch.no_grad():
            all_item_ids = torch.LongTensor(list(range(item_count)))
            all_item_embs = model.get_item_embedding(all_item_ids).detach().numpy()
            normalized_item_embs = all_item_embs / np.linalg.norm(all_item_embs, axis=1, keepdims=True)
            
            # ä¿å­˜ç‰©å“åµŒå…¥å­—å…¸ï¼Œä½¿ç”¨åŸå§‹ID
            item_embeddings = {item_id_reverse_map[idx]: emb for idx, emb in enumerate(normalized_item_embs)}
            with open(item_emb_path, 'wb') as f:
                pickle.dump(item_embeddings, f)
        
        # è®¡ç®—ç”¨æˆ·åµŒå…¥
        user_embeddings = {}
        max_seq_len = 30
        
        with torch.no_grad():
            for mapped_user_id, hist_items in tqdm(user_hist_dict.items(), desc="Computing user embeddings"):
                if not hist_items:
                    continue
                    
                # æœ€å¤šä½¿ç”¨æœ€è¿‘30ä¸ªäº¤äº’
                hist_items = hist_items[-max_seq_len:] if len(hist_items) > max_seq_len else hist_items
                hist_len = len(hist_items)
                
                # å°†å†å²äº¤äº’è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥
                hist_tensor = torch.LongTensor(hist_items + [0] * (max_seq_len - hist_len))
                hist_tensor = hist_tensor.unsqueeze(0)  # å¢åŠ æ‰¹æ¬¡ç»´åº¦
                user_tensor = torch.LongTensor([mapped_user_id])
                seq_len = torch.LongTensor([hist_len])
                
                # è·å–ç”¨æˆ·åµŒå…¥
                try:
                    user_emb = model.get_user_embedding(user_tensor, hist_tensor, seq_len).numpy()
                    # ä¿å­˜æ—¶ä½¿ç”¨åŸå§‹ID
                    original_user_id = user_id_reverse_map[mapped_user_id]
                    user_embeddings[original_user_id] = user_emb.squeeze() / np.linalg.norm(user_emb)
                except Exception as e:
                    print(f"[get_youtube_recall] âš ï¸ Error processing embedding for user {mapped_user_id}: {str(e)}")  # å¤„ç†ç”¨æˆ·åµŒå…¥æ—¶å‡ºé”™
                    continue
        
        # ä¿å­˜ç”¨æˆ·åµŒå…¥
        with open(user_emb_path, 'wb') as f:
            pickle.dump(user_embeddings, f)
    
    # åœ¨ç”ŸæˆåµŒå…¥åæ·»åŠ 
    if user_embeddings and item_embeddings:
        # éšæœºæŠ½å–5ä¸ªç”¨æˆ·å’Œ5ä¸ªç‰©å“æ£€æŸ¥åµŒå…¥è´¨é‡
        sample_users = list(user_embeddings.keys())[:5]
        sample_items = list(item_embeddings.keys())[:5]
        
        print("\n[DEBUG] Sample embedding quality check")  # æ ·æœ¬åµŒå…¥è´¨é‡æ£€æŸ¥
        for user_id in sample_users:
            user_emb = user_embeddings[user_id]
            print(f"User {user_id} embedding norm: {np.linalg.norm(user_emb):.4f}")  # ç”¨æˆ·åµŒå…¥èŒƒæ•°
            
            # æ£€æŸ¥ä¸æ ·æœ¬ç‰©å“çš„ç›¸ä¼¼åº¦
            for item_id in sample_items:
                item_emb = item_embeddings[item_id]
                sim = np.dot(user_emb, item_emb) / (np.linalg.norm(user_emb) * np.linalg.norm(item_emb))
                print(f"  Similarity to item {item_id}: {sim:.4f}")  # ä¸ç‰©å“...çš„ç›¸ä¼¼åº¦
    
    # å‡†å¤‡å‘é‡æ£€ç´¢
    print("[get_youtube_recall] Using Faiss for vector retrieval...")  # ä½¿ç”¨Faissè¿›è¡Œå‘é‡æ£€ç´¢...
    user_ids = list(user_embeddings.keys())
    user_embs = np.array([user_embeddings[user_id] for user_id in user_ids], dtype=np.float32)
    
    item_ids = list(item_embeddings.keys())
    item_embs = np.array([item_embeddings[item_id] for item_id in item_ids], dtype=np.float32)
    
    # æ„å»ºç´¢å¼•
    index = faiss.IndexFlatIP(embedding_dim)
    index.add(np.ascontiguousarray(item_embs))
    
    # ä¸ºéªŒè¯é›†ä¸­çš„ç”¨æˆ·ç”Ÿæˆå¬å›ç»“æœ
    user_recall_items_dict = {}
    topk = recall_num  # æ¯ä¸ªç”¨æˆ·å¬å›æŒ‡å®šæ•°é‡çš„æ–‡ç« 
    
    # æ‰§è¡Œå‘é‡æ£€ç´¢
    sim, idx = index.search(np.ascontiguousarray(user_embs), topk)
    
    # æ„å»ºç”¨æˆ·å¬å›ç»“æœ
    for i, user_id in enumerate(user_ids):
        item_list = []
        for j, item_idx in enumerate(idx[i]):
            if item_idx < len(item_ids):
                item_id = item_ids[item_idx]
                score = float(sim[i][j])
                item_list.append((item_id, score))
        user_recall_items_dict[user_id] = item_list
    
    # ä¿å­˜å¬å›ç»“æœ
    with open(cache_path, 'wb') as f:
        pickle.dump(user_recall_items_dict, f)
    
    print(f"[get_youtube_recall] âœ… Recall results saved to: {cache_path}")  # å¬å›ç»“æœå·²ä¿å­˜è‡³
    return user_recall_items_dict

def metrics_recall_at_k(user_recall_items_dict, val_df, k):
    """è®¡ç®—æŒ‡å®škå€¼çš„å¬å›ç‡"""
    val_user_items = dict(zip(val_df['user_id'], val_df['click_article_id']))
    covered_users = set(user_recall_items_dict.keys()) & set(val_user_items.keys())
    hit = 0
    for user in covered_users:
        true_item = val_user_items[user]
        recall_items = [x[0] for x in user_recall_items_dict[user][:k]]
        if true_item in recall_items:
            hit += 1
    recall = round(hit / len(covered_users), 5) if covered_users else 0
    return recall

def offline_evaluate_multi(use_cache=True, recall_num=50, epochs=10, batch_size=32, embedding_dim=32):
    print(f"\nğŸ“‚ Current cache directory: {cache_dir}")  # å½“å‰ä½¿ç”¨çš„ç¼“å­˜ç›®å½•
    
    # æ£€æŸ¥ç°æœ‰ç¼“å­˜æ–‡ä»¶
    if os.path.exists(cache_dir):
        cache_files = os.listdir(cache_dir)
        print("\nExisting cache files:")  # ç°æœ‰ç¼“å­˜æ–‡ä»¶
        for file in cache_files:
            file_path = os.path.join(cache_dir, file)
            file_time = datetime.fromtimestamp(os.path.getmtime(file_path))
            print(f"  - {file} (updated: {file_time})")  # æ›´æ–°æ—¶é—´
    
    # ç»Ÿä¸€ä½¿ç”¨ä¸€ä¸ªæ¨¡å‹æ–‡ä»¶å
    model_cache = os.path.join(cache_dir, 'youtube_model.pth')  # ä½¿ç”¨å®é™…ä¿å­˜çš„æ–‡ä»¶å
    
    # å¦‚æœæ¨¡å‹ç¼“å­˜å­˜åœ¨ï¼Œç›´æ¥åŠ è½½
    if use_cache and os.path.exists(model_cache):
        print(f"\nâœ… Found model cache: {model_cache}")  # å‘ç°æ¨¡å‹ç¼“å­˜
        print(f"   Cache time: {datetime.fromtimestamp(os.path.getmtime(model_cache))}")  # ç¼“å­˜æ—¶é—´
        print("âœ… Model cache found")  # å·²æˆåŠŸæ‰¾åˆ°æ¨¡å‹ç¼“å­˜
    
    os.makedirs(cache_dir, exist_ok=True)
    
    # Step 1: åŠ è½½æ•°æ® & åˆ’åˆ†è®­ç»ƒ/éªŒè¯
    print("ğŸ“Œ Loading training data and splitting validation set...")  # åŠ è½½è®­ç»ƒæ•°æ®å¹¶åˆ’åˆ†éªŒè¯é›†...
    all_click_df = get_all_click_df(data_path=data_path, offline=True)
    train_df, val_df = split_train_val(all_click_df)
    print(f"âœ… Train records: {len(train_df)}, validation records: {len(val_df)}")  # è®­ç»ƒé›†...éªŒè¯é›†...
    
    # Step 2: å‡†å¤‡ç›¸å…³æ•°æ®
    item_info_df = get_item_info_df(data_path)
    item_type_dict, item_words_dict, item_created_time_dict = get_item_info_dict(item_info_df)
    user_item_time_dict = get_user_item_time_dict(train_df)
    item_topk_click = get_item_topk_click(train_df, k=50)
    
    # Step 3: åŠ è½½å¹¶æŠ½æ · embeddingï¼Œæ„å»º embedding ç›¸ä¼¼åº¦
    print("ğŸš€ Step 3: Loading article embeddings and sampling for similarity")  # åŠ è½½æ–‡ç«  embedding å¹¶æŠ½æ ·æ„å»ºç›¸ä¼¼åº¦
    emb_sample_n = 1000
    item_emb_df = pd.read_csv(data_path + '/articles_emb.csv').sample(n=emb_sample_n, random_state=42)
    emb_item_ids = set(item_emb_df['article_id'])
    click_df_for_emb = train_df[train_df['click_article_id'].isin(emb_item_ids)]
    
    # embedding_sim åªç”¨ click_df_for_embï¼Œè€Œä¸è¦æ±¡æŸ“ä¸»æµç¨‹çš„ train_df
    emb_i2i_sim = embdding_sim(click_df_for_emb, item_emb_df, save_path=cache_dir, topk=10)
    print(f"âœ… Step 3: Embedding similarity computation completed")  # å®Œæˆ embedding ç›¸ä¼¼åº¦è®¡ç®—
    
    # Step 4: é¦–å…ˆç”ŸæˆYouTubeDNNå¬å›å¹¶æå–ç”¨æˆ·åµŒå…¥
    print("ğŸ”„ Step 4.1: Generate YouTubeDNN recall and extract user embeddings...")  # ç”ŸæˆYouTubeDNNå¬å›å¹¶æå–ç”¨æˆ·åµŒå…¥...
    
    # åˆå¹¶æ•°æ®å¹¶å¢åŠ æ—¶é—´æˆ³æ’åº
    all_df = pd.concat([train_df, val_df], ignore_index=True)
    all_df = all_df.sort_values('click_timestamp')
    
    # å…ˆç”ŸæˆYouTubeDNNçš„å¬å›ç»“æœå’ŒåµŒå…¥
    youtube_recall_dict = youtubednn_u2i_dict(
        data=all_df,
        save_path=cache_dir,
        topk=recall_num,
        epochs=epochs,
        batch_size=batch_size,
        embedding_dim=embedding_dim
    )
    
    # ç„¶ååŠ è½½ç”Ÿæˆçš„ç”¨æˆ·åµŒå…¥
    user_emb_path = os.path.join(cache_dir, 'youtube_embeddings.pkl')  # æ³¨æ„è¿™é‡Œæ”¹ç”¨æ­£ç¡®çš„æ–‡ä»¶å
    if os.path.exists(user_emb_path):
        print(f"[offline_evaluate_multi] âœ… Loaded user embeddings: {user_emb_path}")  # åŠ è½½ç”¨æˆ·åµŒå…¥
        with open(user_emb_path, 'rb') as f:
            cache_data = pickle.load(f)
            user_embeddings = cache_data['user_embeddings']
    else:
        print("[offline_evaluate_multi] âš ï¸ User embedding file not found")  # æ— æ³•æ‰¾åˆ°ç”¨æˆ·åµŒå…¥æ–‡ä»¶
        user_embeddings = {}
    
    # Step 5: ç”Ÿæˆä¼ ç»ŸItemCFå¬å›
    print("ğŸ”„ Step 4.2: Generating traditional ItemCF recall...")  # ç”Ÿæˆä¼ ç»ŸItemCFå¬å›...
    i2i_sim = itemcf_sim(
        train_df,
        item_created_time_dict,
        save_path=os.path.join(cache_dir, 'itemcf_i2i_sim.pkl'),
        use_cache=use_cache
    )
    
    itemcf_recall_dict = generate_itemcf_recall_dict(
        val_df=val_df,
        user_item_time_dict=user_item_time_dict,
        i2i_sim=i2i_sim,
        sim_item_topk=10,
        recall_item_num=recall_num,
        item_topk_click=item_topk_click,
        item_created_time_dict=item_created_time_dict,
        emb_i2i_sim=emb_i2i_sim,  # æ³¨æ„è¿™é‡Œä¼šèåˆä¸€äº›embeddingä¿¡æ¯
        save_path=os.path.join(cache_dir, 'itemcf_recall_dict.pkl'),
        use_cache=use_cache
    )
    
    # Step 6: ç”ŸæˆåŸºäºEmbeddingçš„ItemCFå¬å›
    print("ğŸ”„ Step 4.3: Generating embedding-based ItemCF recall...")  # ç”ŸæˆåŸºäºEmbeddingçš„ItemCFå¬å›...
    itemcf_emb_recall_dict = generate_itemcf_embedding_recall_dict(
        val_df=val_df,
        emb_i2i_sim=emb_i2i_sim,  # ç›´æ¥ä½¿ç”¨embeddingç›¸ä¼¼åº¦
        user_item_time_dict=user_item_time_dict,
        sim_item_topk=10,
        recall_item_num=recall_num,
        item_topk_click=item_topk_click,
        item_created_time_dict=item_created_time_dict,
        save_path=os.path.join(cache_dir, 'itemcf_emb_recall_dict.pkl'),
        use_cache=use_cache
    )
    
    # Step 7: ç”ŸæˆåŸºäºEmbeddingçš„UserCFå¬å›
    print("\nğŸ”„ Step 4.4: Generating embedding-based UserCF recall...")  # ç”ŸæˆåŸºäºEmbeddingçš„UserCFå¬å›...
    
    # æ£€æŸ¥ç”¨æˆ·åµŒå…¥çš„å¯ç”¨æ€§
    if not user_embeddings:
        print("âš ï¸ Warning: user embeddings are empty")  # è­¦å‘Šï¼šç”¨æˆ·åµŒå…¥ä¸ºç©º
        print(f"User embedding count: {len(user_embeddings)}")  # ç”¨æˆ·åµŒå…¥æ•°é‡
    
    u2u_emb_sim = u2u_embedding_sim(
        click_df=val_df,
        user_emb_dict=user_embeddings,
        save_path=os.path.join(cache_dir, 'youtube_u2u_sim.pkl'),
        topk=20,
        use_cache=use_cache
    )
    
    # æ‰“å°ç”¨æˆ·ç›¸ä¼¼åº¦çŸ©é˜µçš„ç»Ÿè®¡ä¿¡æ¯
    print(f"\nUser similarity matrix stats:")  # ç”¨æˆ·ç›¸ä¼¼åº¦çŸ©é˜µç»Ÿè®¡
    print(f"Total users: {len(val_df['user_id'].unique())}")  # æ€»ç”¨æˆ·æ•°
    print(f"Users in similarity matrix: {len(u2u_emb_sim)}")  # ç›¸ä¼¼åº¦çŸ©é˜µä¸­çš„ç”¨æˆ·æ•°
    
    print("\nğŸ”„ Step 4.5: Generating embedding-based UserCF recall results...")  # ç”ŸæˆåŸºäºEmbeddingçš„UserCFçš„å¬å›ç»“æœ...
    usercf_emb_recall_dict = generate_usercf_recall_dict(
        click_df=val_df,
        user_item_time_dict=user_item_time_dict,
        u2u_sim=u2u_emb_sim,
        sim_user_topk=10,
        recall_item_num=recall_num,
        item_topk_click=item_topk_click,
        item_created_time_dict=item_created_time_dict,
        emb_i2i_sim=i2i_sim,
        save_path=cache_dir,  # æ·»åŠ ç¼“å­˜è·¯å¾„
        use_cache=use_cache   # ä½¿ç”¨ç¼“å­˜å‚æ•°
    )
    
    # Step 8: è¯„ä¼°å„ç§å¬å›æ–¹æ³•
    print("\nğŸ“Š Evaluating traditional ItemCF recall...")  # è¯„ä¼°ä¼ ç»ŸItemCFå¬å›æ•ˆæœ...
    itemcf_recalls = {k: metrics_recall_at_k(itemcf_recall_dict, val_df, k) for k in [10,20,30,40,50]}
    itemcf_mrr = metrics_mrr(itemcf_recall_dict, val_df, topk=5)
    
    print("\nğŸ“Š Evaluating embedding-based ItemCF recall...")  # è¯„ä¼°åŸºäºEmbeddingçš„ItemCFå¬å›æ•ˆæœ...
    itemcf_emb_recalls = {k: metrics_recall_at_k(itemcf_emb_recall_dict, val_df, k) for k in [10,20,30,40,50]}
    itemcf_emb_mrr = metrics_mrr(itemcf_emb_recall_dict, val_df, topk=5)
    
    print("\nğŸ“Š Evaluating embedding-based UserCF recall...")  # è¯„ä¼°åŸºäºEmbeddingçš„UserCFå¬å›æ•ˆæœ...
    usercf_emb_recalls = {k: metrics_recall_at_k(usercf_emb_recall_dict, val_df, k) for k in [10,20,30,40,50]}
    usercf_emb_mrr = metrics_mrr(usercf_emb_recall_dict, val_df, topk=5)
    
    print("\nğŸ“Š Evaluating YouTubeDNN recall...")  # è¯„ä¼°YouTubeDNNå¬å›æ•ˆæœ...
    youtube_recalls = {k: metrics_recall_at_k(youtube_recall_dict, val_df, k) for k in [10,20,30,40,50]}
    youtube_mrr = metrics_mrr(youtube_recall_dict, val_df, topk=5)
    
    # Step 9: åˆå¹¶å¬å›ç»“æœ
    print("\nğŸ”„ Combining multiple recall results...")  # åˆå¹¶å¤šè·¯å¬å›ç»“æœ...
    # åˆ›å»ºå¤šè·¯å¬å›å­—å…¸
    user_multi_recall_dict = {
        'itemcf': itemcf_recall_dict,
        'itemcf_emb': itemcf_emb_recall_dict,
        'usercf_emb': usercf_emb_recall_dict,
        'youtube': youtube_recall_dict
    }
    
    # è®¾ç½®å„è·¯å¬å›çš„æƒé‡
    weight_dict = {
        'itemcf': 1.0,
        'itemcf_emb': 0.8,
        'usercf_emb': 0.9,
        'youtube': 1.2
    }
    
    # åˆå¹¶å¬å›ç»“æœ
    final_recall_dict = combine_recall_results(
        user_multi_recall_dict=user_multi_recall_dict,
        weight_dict=weight_dict,
        topk=recall_num,
        save_path=os.path.join(cache_dir, 'final_recall_dict.pkl')
    )
    
    # Step 10: è¯„ä¼°åˆå¹¶åçš„å¬å›æ•ˆæœ
    print("\nğŸ“Š Evaluating combined recall...")  # è¯„ä¼°åˆå¹¶åçš„å¬å›æ•ˆæœ...
    final_recalls = {k: metrics_recall_at_k(final_recall_dict, val_df, k) for k in [10,20,30,40,50]}
    final_mrr = metrics_mrr(final_recall_dict, val_df, topk=5)
    
    # æ‰“å°å®Œæ•´çš„ç»“æœè¡¨æ ¼
    print("\nğŸ“Š Final comparison:")  # æœ€ç»ˆç»“æœå¯¹æ¯”
    print(f"{'Recall method':<15} {'Recall@10':<10} {'Recall@20':<10} {'Recall@30':<10} {'Recall@40':<10} {'Recall@50':<10} {'MRR@5':<10}")  # å¬å›æ–¹æ³• è¡¨å¤´
    print("-" * 85)
    
    methods = {
        'ItemCF': (itemcf_recalls, itemcf_mrr),
        'ItemCF-Emb': (itemcf_emb_recalls, itemcf_emb_mrr),
        'UserCF-Emb': (usercf_emb_recalls, usercf_emb_mrr),
        'YouTubeDNN': (youtube_recalls, youtube_mrr),
        'Combined': (final_recalls, final_mrr)  # åˆå¹¶ç»“æœ
    }
    
    for method_name, (recalls, mrr) in methods.items():
        print(f"{method_name:<15} "
              f"{recalls[10]:<10.5f} "
              f"{recalls[20]:<10.5f} "
              f"{recalls[30]:<10.5f} "
              f"{recalls[40]:<10.5f} "
              f"{recalls[50]:<10.5f} "
              f"{mrr:<10.5f}")

    # è¿”å›ç»“æœæ—¶åŒ…å«æ‰€æœ‰recallå€¼
    return {
        'itemcf': {'recalls': itemcf_recalls, 'mrr': itemcf_mrr},
        'itemcf_emb': {'recalls': itemcf_emb_recalls, 'mrr': itemcf_emb_mrr},
        'usercf_emb': {'recalls': usercf_emb_recalls, 'mrr': usercf_emb_mrr},
        'youtube': {'recalls': youtube_recalls, 'mrr': youtube_mrr},
        'combined': {'recalls': final_recalls, 'mrr': final_mrr}
    }

if __name__ == '__main__':
    # é…ç½®å‚æ•°
    use_cache = False  # å¼ºåˆ¶é‡æ–°è®­ç»ƒ
    recall_num = 50   # å¬å›æ•°é‡
    epochs = 20        # å¢åŠ è®­ç»ƒè½®æ•°
    batch_size = 128   # å¢åŠ æ‰¹æ¬¡å¤§å°
    embedding_dim = 64  # å¢åŠ åµŒå…¥ç»´åº¦
    
    # æ£€æŸ¥è®¾å¤‡
    device = "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Using device: {device}")  # ä½¿ç”¨è®¾å¤‡
    
    # è¿è¡Œå¤šæºå¬å›è¯„ä¼°
    results = offline_evaluate_multi(
        use_cache=use_cache,  # å¼ºåˆ¶é‡æ–°è®­ç»ƒ
        recall_num=recall_num,
        epochs=epochs,        # å¢åŠ è®­ç»ƒè½®æ•°
        batch_size=batch_size,   # å¢åŠ æ‰¹æ¬¡å¤§å°
        embedding_dim=embedding_dim  # å¢åŠ åµŒå…¥ç»´åº¦
    ) 